package codegen

import (
	"fmt"
	"io"
	"os"
	"path/filepath"
	"sort"
	"strconv"
	"strings"

	"github.com/johnkerl/pgpg/tmp/ir"
)

// Generator converts IR into language-specific code.
type Generator interface {
	Generate(ir *ir.IR, outDir string) error
}

type goGenerator struct {
	log io.Writer
}

// NewGo returns a Go code generator.
func NewGo(log io.Writer) (Generator, error) {
	if log == nil {
		log = io.Discard
	}
	return &goGenerator{log: log}, nil
}

func (g *goGenerator) Generate(doc *ir.IR, outDir string) error {
	if doc == nil {
		return fmt.Errorf("codegen: nil IR")
	}
	if outDir == "" {
		return fmt.Errorf("codegen: output directory is empty")
	}
	if err := os.MkdirAll(outDir, 0o755); err != nil {
		return fmt.Errorf("codegen: create output dir: %w", err)
	}

	pkg := filepath.Base(outDir)
	if pkg == "." || pkg == string(filepath.Separator) {
		pkg = "parser"
	}

	pkgName := sanitizePackageName(pkg)

	docFile := filepath.Join(outDir, "doc.go")
	docContent := fmt.Sprintf(`// Code generated by PGPG. DO NOT EDIT.
//
// Version: %s
// Source: %s
package %s

// This package contains generated lexer and parser stubs.
`, doc.Version, doc.Meta.Source, pkgName)
	if err := os.WriteFile(docFile, []byte(docContent), 0o644); err != nil {
		return fmt.Errorf("codegen: write %s: %w", docFile, err)
	}
	fmt.Fprintf(g.log, "wrote %s\n", docFile)

	lexerFile := filepath.Join(outDir, "lexer.go")
	lexerContent := fmt.Sprintf(`// Code generated by PGPG. DO NOT EDIT.
package %s

// TokenType identifies the category of a lexeme.
type TokenType string

const (
	// TokenEOF is the end-of-input marker expected by the parser.
	TokenEOF TokenType = "$"
)

// Token is a single lexeme with source position.
type Token struct {
	Type   TokenType
	Value  string
	Offset int
	Line   int
	Column int
}

// Lexer is the interface for tokenization.
type Lexer interface {
	Next() (Token, error)
	Peek() (Token, error)
}

// SliceLexer is a minimal lexer for pre-tokenized input.
type SliceLexer struct {
	tokens []Token
	pos    int
}

// NewSliceLexer returns a lexer over a fixed slice of tokens.
func NewSliceLexer(tokens []Token) *SliceLexer {
	return &SliceLexer{tokens: tokens}
}

// Next returns the next token, or EOF when exhausted.
func (l *SliceLexer) Next() (Token, error) {
	if l.pos >= len(l.tokens) {
		return Token{Type: TokenEOF}, nil
	}
	tok := l.tokens[l.pos]
	l.pos++
	return tok, nil
}

// Peek returns the next token without consuming it.
func (l *SliceLexer) Peek() (Token, error) {
	if l.pos >= len(l.tokens) {
		return Token{Type: TokenEOF}, nil
	}
	return l.tokens[l.pos], nil
}

// TODO: generated lexer implementation will go here.
`, pkgName)
	if err := os.WriteFile(lexerFile, []byte(lexerContent), 0o644); err != nil {
		return fmt.Errorf("codegen: write %s: %w", lexerFile, err)
	}
	fmt.Fprintf(g.log, "wrote %s\n", lexerFile)

	parserFile := filepath.Join(outDir, "parser.go")
	actionsLiteral, gotosLiteral, prodsLiteral, actionIDsLiteral, prodCount := renderParseTables(doc.Parser)
	registryEntries, actionStubs := renderSemanticActions(doc.Parser.SemanticActions)
	parserContent := fmt.Sprintf(`// Code generated by PGPG. DO NOT EDIT.
package %s

import "fmt"

// ActionType indicates the parser action kind.
type ActionType int

const (
	ActionError ActionType = iota
	ActionShift
	ActionReduce
	ActionAccept
	ActionDiscard
)

// Action is a table entry for terminals.
type Action struct {
	Kind  ActionType
	Value int
}

// Production is a single grammar rule used by reductions.
type Production struct {
	LHS      string
	RHSCount int
}

// SemanticAction builds an AST value from a reduction.
type SemanticAction func(values []any) (any, error)

// Result captures parser output and diagnostics.
type Result struct {
	AST  any
	Warn []string
}

// Parser consumes a token stream.
type Parser interface {
	Parse() (Result, error)
}

type parserImpl struct {
	lexer  Lexer
	tables parseTables
	opts   ParserOptions
}

type parseTables struct {
	actions map[int]map[TokenType]Action
	gotos   map[int]map[string]int
	prods   []Production
	actionsByProd []SemanticAction
	// actionIDs contains semantic action IDs aligned with productions.
	actionIDs []string
}

// ErrorHandler can recover from an error by returning an alternate action.
type ErrorHandler func(state int, tok Token) (Action, bool)

// ParserOptions configures the parser behavior.
type ParserOptions struct {
	OnError ErrorHandler
}

// NewParser returns a parser for the provided lexer.
func NewParser(lex Lexer) (Parser, error) {
	return NewParserWithOptions(lex, ParserOptions{})
}

// NewParserWithOptions returns a parser configured with custom options.
func NewParserWithOptions(lex Lexer, opts ParserOptions) (Parser, error) {
	tables := buildTables()
	bindSemanticActions(&tables)
	return &parserImpl{
		lexer:  lex,
		tables: tables,
		opts:   opts,
	}, nil
}

// Parse runs a table-driven shift/reduce loop.
func (p *parserImpl) Parse() (Result, error) {
	if p.lexer == nil {
		return Result{}, fmt.Errorf("parser: nil lexer")
	}

	stateStack := []int{0}
	valueStack := []any{}
	var tok Token
	var err error
	var hasLookahead bool

	for {
		if !hasLookahead {
			tok, err = p.lexer.Next()
			if err != nil {
				return Result{}, err
			}
			hasLookahead = true
		}

		state := stateStack[len(stateStack)-1]
		action := p.tables.action(state, tok.Type)
		switch action.Kind {
		case ActionShift:
			stateStack = append(stateStack, action.Value)
			valueStack = append(valueStack, tok)
			hasLookahead = false
		case ActionReduce:
			if action.Value < 0 || action.Value >= len(p.tables.prods) {
				return Result{}, fmt.Errorf("parser: invalid production %d", action.Value)
			}
			prod := p.tables.prods[action.Value]
			if prod.RHSCount > len(stateStack)-1 || prod.RHSCount > len(valueStack) {
				return Result{}, fmt.Errorf("parser: stack underflow on reduce")
			}

			var rhsVals []any
			if prod.RHSCount > 0 {
				rhsVals = append(rhsVals, valueStack[len(valueStack)-prod.RHSCount:]...)
				valueStack = valueStack[:len(valueStack)-prod.RHSCount]
			}
			stateStack = stateStack[:len(stateStack)-prod.RHSCount]

			nextState, ok := p.tables.goTo(stateStack[len(stateStack)-1], prod.LHS)
			if !ok {
				return Result{}, fmt.Errorf("parser: missing goto for %s", prod.LHS)
			}
			stateStack = append(stateStack, nextState)

			var node any
			if action.Value < len(p.tables.actionsByProd) && p.tables.actionsByProd[action.Value] != nil {
				node, err = p.tables.actionsByProd[action.Value](rhsVals)
				if err != nil {
					return Result{}, err
				}
			}
			valueStack = append(valueStack, node)
		case ActionAccept:
			if len(valueStack) > 0 {
				return Result{AST: valueStack[len(valueStack)-1]}, nil
			}
			return Result{}, nil
		case ActionDiscard:
			hasLookahead = false
			continue
		default:
			if p.opts.OnError != nil {
				if nextAction, ok := p.opts.OnError(state, tok); ok {
					action = nextAction
					hasLookahead = true
					continue
				}
			}
			return Result{}, fmt.Errorf("parser: unexpected token %q at %d:%d", tok.Value, tok.Line, tok.Column)
		}
	}
}

func (t parseTables) action(state int, tokType TokenType) Action {
	if row, ok := t.actions[state]; ok {
		if action, ok := row[tokType]; ok {
			return action
		}
	}
	return Action{Kind: ActionError}
}

func (t parseTables) goTo(state int, nonterminal string) (int, bool) {
	if row, ok := t.gotos[state]; ok {
		next, ok := row[nonterminal]
		return next, ok
	}
	return 0, false
}

// SyncSet defines tokens that can be used as synchronization points.
type SyncSet map[TokenType]struct{}

// PanicModeHandler returns an error handler that discards tokens until a sync
// token (or EOF) is observed. This is a placeholder recovery strategy.
func PanicModeHandler(sync SyncSet, maxSkips int) ErrorHandler {
	skips := 0
	return func(_ int, tok Token) (Action, bool) {
		if maxSkips > 0 && skips >= maxSkips {
			return Action{}, false
		}
		if tok.Type == TokenEOF {
			return Action{Kind: ActionAccept}, true
		}
		if _, ok := sync[tok.Type]; ok {
			// TODO: attempt to re-sync with real tables once they exist.
			return Action{Kind: ActionDiscard}, true
		}
		skips++
		return Action{Kind: ActionDiscard}, true
	}
}

// buildTables returns generated parse tables from embedded IR data.
func buildTables() parseTables {
	prods := %s
	actionIDs := %s
	actions := %s
	gotos := %s
	return parseTables{
		actions:       actions,
		gotos:         gotos,
		prods:         prods,
		actionsByProd: make([]SemanticAction, %d),
		actionIDs:     actionIDs,
	}
}

func bindSemanticActions(t *parseTables) {
	registry := semanticActionRegistry()
	for i, actionID := range t.actionIDs {
		if actionID == "" {
			continue
		}
		if action, ok := registry[actionID]; ok {
			t.actionsByProd[i] = action
		}
	}
}

func semanticActionRegistry() map[string]SemanticAction {
	return map[string]SemanticAction{
%s
	}
}

%s
`, pkgName, prodsLiteral, actionIDsLiteral, actionsLiteral, gotosLiteral, prodCount, registryEntries, actionStubs)
	if err := os.WriteFile(parserFile, []byte(parserContent), 0o644); err != nil {
		return fmt.Errorf("codegen: write %s: %w", parserFile, err)
	}
	fmt.Fprintf(g.log, "wrote %s\n", parserFile)
	return nil
}

func sanitizePackageName(name string) string {
	if name == "" {
		return "parser"
	}
	out := make([]rune, 0, len(name))
	for i, r := range name {
		if (r >= 'a' && r <= 'z') || (r >= 'A' && r <= 'Z') || r == '_' || (i > 0 && r >= '0' && r <= '9') {
			out = append(out, r)
		}
	}
	if len(out) == 0 {
		return "parser"
	}
	if out[0] >= 'A' && out[0] <= 'Z' {
		out[0] = out[0] - 'A' + 'a'
	}
	return string(out)
}

func renderParseTables(pir ir.ParserIR) (actions string, gotos string, prods string, actionIDs string, prodCount int) {
	if len(pir.Productions) == 0 {
		return "map[int]map[TokenType]Action{}", "map[int]map[string]int{}", "[]Production{}", "[]string{}", 0
	}

	prodsBuilder := strings.Builder{}
	prodsBuilder.WriteString("[]Production{\n")
	for _, prod := range pir.Productions {
		prodsBuilder.WriteString(fmt.Sprintf("\t{LHS: %s, RHSCount: %d},\n", strconv.Quote(prod.LHS), prod.RHSCount))
	}
	prodsBuilder.WriteString("}")

	actionIDsBuilder := strings.Builder{}
	actionIDsBuilder.WriteString("[]string{\n")
	for _, prod := range pir.Productions {
		actionIDsBuilder.WriteString(fmt.Sprintf("\t%s,\n", strconv.Quote(prod.ActionID)))
	}
	actionIDsBuilder.WriteString("}")

	actionsBuilder := strings.Builder{}
	actionsBuilder.WriteString("map[int]map[TokenType]Action{\n")
	actionsByState := map[int][]ir.ActionEntryIR{}
	for _, entry := range pir.Actions {
		actionsByState[entry.State] = append(actionsByState[entry.State], entry)
	}
	stateKeys := make([]int, 0, len(actionsByState))
	for state := range actionsByState {
		stateKeys = append(stateKeys, state)
	}
	sort.Ints(stateKeys)
	for _, state := range stateKeys {
		actionsBuilder.WriteString(fmt.Sprintf("\t%d: {\n", state))
		entries := actionsByState[state]
		sort.Slice(entries, func(i, j int) bool {
			return entries[i].Terminal < entries[j].Terminal
		})
		for _, entry := range entries {
			actionLiteral := renderActionLiteral(entry)
			tokenLiteral := renderTokenTypeLiteral(entry.Terminal)
			if actionLiteral == "" {
				continue
			}
			actionsBuilder.WriteString(fmt.Sprintf("\t\t%s: %s,\n", tokenLiteral, actionLiteral))
		}
		actionsBuilder.WriteString("\t},\n")
	}
	actionsBuilder.WriteString("}")

	gotosBuilder := strings.Builder{}
	gotosBuilder.WriteString("map[int]map[string]int{\n")
	gotosByState := map[int][]ir.GotoEntryIR{}
	for _, entry := range pir.Gotos {
		gotosByState[entry.State] = append(gotosByState[entry.State], entry)
	}
	gotoKeys := make([]int, 0, len(gotosByState))
	for state := range gotosByState {
		gotoKeys = append(gotoKeys, state)
	}
	sort.Ints(gotoKeys)
	for _, state := range gotoKeys {
		gotosBuilder.WriteString(fmt.Sprintf("\t%d: {\n", state))
		entries := gotosByState[state]
		sort.Slice(entries, func(i, j int) bool {
			return entries[i].Nonterminal < entries[j].Nonterminal
		})
		for _, entry := range entries {
			gotosBuilder.WriteString(fmt.Sprintf("\t\t%s: %d,\n", strconv.Quote(entry.Nonterminal), entry.Next))
		}
		gotosBuilder.WriteString("\t},\n")
	}
	gotosBuilder.WriteString("}")

	return actionsBuilder.String(), gotosBuilder.String(), prodsBuilder.String(), actionIDsBuilder.String(), len(pir.Productions)
}

func renderActionLiteral(entry ir.ActionEntryIR) string {
	switch strings.ToLower(entry.Action) {
	case "shift":
		return fmt.Sprintf("Action{Kind: ActionShift, Value: %d}", entry.Value)
	case "reduce":
		return fmt.Sprintf("Action{Kind: ActionReduce, Value: %d}", entry.Value)
	case "accept":
		return "Action{Kind: ActionAccept}"
	case "discard":
		return "Action{Kind: ActionDiscard}"
	case "error", "":
		return "Action{Kind: ActionError}"
	default:
		return ""
	}
}

func renderTokenTypeLiteral(terminal string) string {
	if terminal == "" {
		return "TokenEOF"
	}
	if terminal == "$" {
		return "TokenEOF"
	}
	return fmt.Sprintf("TokenType(%s)", strconv.Quote(terminal))
}

func renderSemanticActions(actions []ir.SemanticActionIR) (registryEntries string, stubs string) {
	if len(actions) == 0 {
		return "", "// No semantic actions defined.\n"
	}

	registry := strings.Builder{}
	stubBuilder := strings.Builder{}
	seen := map[string]bool{}
	for _, action := range actions {
		if action.ID == "" || seen[action.ID] {
			continue
		}
		seen[action.ID] = true
		funcName := sanitizeActionID(action.ID)
		registry.WriteString(fmt.Sprintf("\t\t%s: %s,\n", strconv.Quote(action.ID), funcName))

		stubBuilder.WriteString(fmt.Sprintf("func %s(values []any) (any, error) {\n", funcName))
		if action.Note != "" {
			stubBuilder.WriteString(fmt.Sprintf("\t// NOTE: %s\n", escapeComment(action.Note)))
		}
		stubBuilder.WriteString("\t_ = values\n")
		stubBuilder.WriteString("\treturn nil, nil\n")
		stubBuilder.WriteString("}\n\n")
	}

	return registry.String(), stubBuilder.String()
}

func sanitizeActionID(id string) string {
	if id == "" {
		return "action"
	}
	var out []rune
	for _, r := range id {
		if (r >= 'a' && r <= 'z') || (r >= 'A' && r <= 'Z') || r == '_' || (len(out) > 0 && r >= '0' && r <= '9') {
			out = append(out, r)
		}
	}
	if len(out) == 0 {
		return "action"
	}
	if out[0] >= '0' && out[0] <= '9' {
		out = append([]rune{'a'}, out...)
	}
	return "action_" + string(out)
}

func escapeComment(note string) string {
	return strings.ReplaceAll(note, "\n", " ")
}
