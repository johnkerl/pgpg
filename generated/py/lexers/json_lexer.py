# Generated by lexgen_code.py from lexgen JSON tables. Do not edit.
from __future__ import annotations

from typing import Optional

from runtime.token import Token, Location, new_eof_token, new_error_token, new_token


START_STATE = 0

TRANSITIONS: dict[int, list[tuple[int, int, int]]] = {

    0: [

        (9, 9, 1),

        (10, 10, 2),

        (13, 13, 3),

        (32, 32, 4),

        (34, 34, 5),

        (44, 44, 6),

        (45, 45, 7),

        (48, 48, 8),

        (49, 57, 9),

        (58, 58, 10),

        (91, 91, 11),

        (93, 93, 12),

        (102, 102, 13),

        (110, 110, 14),

        (116, 116, 15),

        (123, 123, 16),

        (125, 125, 17),

    ],

    5: [

        (32, 33, 18),

        (34, 34, 19),

        (35, 91, 20),

        (92, 92, 21),

        (93, 65535, 22),

    ],

    7: [

        (48, 48, 8),

        (49, 57, 9),

    ],

    8: [

        (46, 46, 23),

        (69, 69, 24),

        (101, 101, 25),

    ],

    9: [

        (46, 46, 23),

        (48, 57, 26),

        (69, 69, 24),

        (101, 101, 25),

    ],

    13: [

        (97, 97, 27),

    ],

    14: [

        (117, 117, 28),

    ],

    15: [

        (114, 114, 29),

    ],

    18: [

        (32, 33, 18),

        (34, 34, 19),

        (35, 91, 20),

        (92, 92, 21),

        (93, 65535, 22),

    ],

    20: [

        (32, 33, 18),

        (34, 34, 19),

        (35, 91, 20),

        (92, 92, 21),

        (93, 65535, 22),

    ],

    21: [

        (34, 34, 30),

        (47, 47, 31),

        (92, 92, 32),

        (98, 98, 33),

        (102, 102, 34),

        (110, 110, 35),

        (114, 114, 36),

        (116, 116, 37),

        (117, 117, 38),

    ],

    22: [

        (32, 33, 18),

        (34, 34, 19),

        (35, 91, 20),

        (92, 92, 21),

        (93, 65535, 22),

    ],

    23: [

        (48, 57, 39),

    ],

    24: [

        (43, 43, 40),

        (45, 45, 41),

        (48, 57, 42),

    ],

    25: [

        (43, 43, 40),

        (45, 45, 41),

        (48, 57, 42),

    ],

    26: [

        (46, 46, 23),

        (48, 57, 26),

        (69, 69, 24),

        (101, 101, 25),

    ],

    27: [

        (108, 108, 43),

    ],

    28: [

        (108, 108, 44),

    ],

    29: [

        (117, 117, 45),

    ],

    30: [

        (32, 33, 18),

        (34, 34, 19),

        (35, 91, 20),

        (92, 92, 21),

        (93, 65535, 22),

    ],

    31: [

        (32, 33, 18),

        (34, 34, 19),

        (35, 91, 20),

        (92, 92, 21),

        (93, 65535, 22),

    ],

    32: [

        (32, 33, 18),

        (34, 34, 19),

        (35, 91, 20),

        (92, 92, 21),

        (93, 65535, 22),

    ],

    33: [

        (32, 33, 18),

        (34, 34, 19),

        (35, 91, 20),

        (92, 92, 21),

        (93, 65535, 22),

    ],

    34: [

        (32, 33, 18),

        (34, 34, 19),

        (35, 91, 20),

        (92, 92, 21),

        (93, 65535, 22),

    ],

    35: [

        (32, 33, 18),

        (34, 34, 19),

        (35, 91, 20),

        (92, 92, 21),

        (93, 65535, 22),

    ],

    36: [

        (32, 33, 18),

        (34, 34, 19),

        (35, 91, 20),

        (92, 92, 21),

        (93, 65535, 22),

    ],

    37: [

        (32, 33, 18),

        (34, 34, 19),

        (35, 91, 20),

        (92, 92, 21),

        (93, 65535, 22),

    ],

    38: [

        (48, 57, 46),

        (65, 70, 47),

        (97, 102, 48),

    ],

    39: [

        (48, 57, 49),

        (69, 69, 24),

        (101, 101, 25),

    ],

    40: [

        (48, 57, 42),

    ],

    41: [

        (48, 57, 42),

    ],

    42: [

        (48, 57, 50),

    ],

    43: [

        (115, 115, 51),

    ],

    44: [

        (108, 108, 52),

    ],

    45: [

        (101, 101, 53),

    ],

    46: [

        (48, 57, 54),

        (65, 70, 55),

        (97, 102, 56),

    ],

    47: [

        (48, 57, 54),

        (65, 70, 55),

        (97, 102, 56),

    ],

    48: [

        (48, 57, 54),

        (65, 70, 55),

        (97, 102, 56),

    ],

    49: [

        (48, 57, 49),

        (69, 69, 24),

        (101, 101, 25),

    ],

    50: [

        (48, 57, 50),

    ],

    51: [

        (101, 101, 57),

    ],

    54: [

        (48, 57, 58),

        (65, 70, 59),

        (97, 102, 60),

    ],

    55: [

        (48, 57, 58),

        (65, 70, 59),

        (97, 102, 60),

    ],

    56: [

        (48, 57, 58),

        (65, 70, 59),

        (97, 102, 60),

    ],

    58: [

        (48, 57, 61),

        (65, 70, 62),

        (97, 102, 63),

    ],

    59: [

        (48, 57, 61),

        (65, 70, 62),

        (97, 102, 63),

    ],

    60: [

        (48, 57, 61),

        (65, 70, 62),

        (97, 102, 63),

    ],

    61: [

        (32, 33, 18),

        (34, 34, 19),

        (35, 91, 20),

        (92, 92, 21),

        (93, 65535, 22),

    ],

    62: [

        (32, 33, 18),

        (34, 34, 19),

        (35, 91, 20),

        (92, 92, 21),

        (93, 65535, 22),

    ],

    63: [

        (32, 33, 18),

        (34, 34, 19),

        (35, 91, 20),

        (92, 92, 21),

        (93, 65535, 22),

    ],

}

ACTIONS: dict[int, str] = {

    1: '!whitespace',

    2: '!whitespace',

    3: '!whitespace',

    4: '!whitespace',

    6: 'comma',

    8: 'number',

    9: 'number',

    10: 'colon',

    11: 'lbracket',

    12: 'rbracket',

    16: 'lcurly',

    17: 'rcurly',

    19: 'string',

    26: 'number',

    39: 'number',

    42: 'number',

    49: 'number',

    50: 'number',

    52: 'null',

    53: 'true',

    57: 'false',

}


def _lookup_transition(state: int, r: int) -> Optional[int]:
    ranges = TRANSITIONS.get(state, [])
    for from_, to_, next_ in ranges:
        if r < from_:
            return None
        if from_ <= r <= to_:
            return next_
    return None


def _is_ignored_token(token_type: str) -> bool:
    return token_type.startswith("!")


class pgpg_JSONLexer:
    """Generated lexer implementing AbstractLexer protocol."""

    def __init__(self, input_text: str) -> None:
        self._input = input_text
        self._length = len(input_text)
        self._index = 0
        self._line = 1
        self._column = 1

    def scan(self) -> Optional[Token]:
        while True:
            if self._index >= self._length:
                return new_eof_token(Location(line=self._line, column=self._column, byte_offset=self._index))

            start_line, start_col = self._line, self._column
            start_index = self._index
            state = START_STATE
            last_accept_state: Optional[int] = None
            last_accept_index = self._index
            last_accept_line, last_accept_col = self._line, self._column

            while self._index < self._length:
                r = ord(self._input[self._index])
                next_state = _lookup_transition(state, r)
                if next_state is None:
                    break
                self._index += 1
                if self._input[self._index - 1] == "\n":
                    self._line += 1
                    self._column = 1
                else:
                    self._column += 1
                state = next_state
                if state in ACTIONS:
                    last_accept_state = state
                    last_accept_index = self._index
                    last_accept_line, last_accept_col = self._line, self._column

            if last_accept_state is None:
                r = ord(self._input[self._index]) if self._index < self._length else 0
                return new_error_token(
                    f"lexer: unrecognized input {chr(r)!r}",
                    Location(line=self._line, column=self._column, byte_offset=self._index),
                )

            lexeme = self._input[start_index:last_accept_index]
            self._index = last_accept_index
            self._line, self._column = last_accept_line, last_accept_col
            token_type = ACTIONS[last_accept_state]

            if _is_ignored_token(token_type):
                continue

            return new_token(lexeme, token_type, Location(line=start_line, column=start_col, byte_offset=start_index))
